we argue that there are three key properties of deep networks that make them efficient in the context of deep learning . we argue that the first is that the full structure of the deep network is the result of a " deepest layer " ( or " hidden layer " ) being initialized with all possible values of the hidden variables . we show that this is not an obstacle to learning , because the initial values of the hidden variables are not random , but are designed to be as close as possible to the full structure we