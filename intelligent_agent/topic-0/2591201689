Abstract This paper addresses issues related to integrating autonomy-enabled, intelligent agents into collaborative, human-machine teams. Interaction with intelligent machine agents capable of making independent, goal-directed decisions in human-machine teaming operations constitutes a major change from traditional human-machine interaction involving teleoperation. Communicating the machine agent’s intent to human counterparts becomes increasingly important as independent machine decisions become subject to human trust and mental models. The authors present findings from their research that suggest existing user display technologies, tailored with context-specific information and the human’s knowledge level of the machine agent’s decision process, can mitigate misperceptions of the appropriateness of agent behavioral responses. This is important because misperceptions on the part of human team members increases the likelihood of trust degradation and unnecessary interventions, ultimately leading to disuse of the agent. Examples of possible issues associated with communicating agent intent, as well as potential implications for trust calibration are provided.